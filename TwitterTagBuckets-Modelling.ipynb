{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import operator\n",
    "from datetime import datetime\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "cluster = Cluster(['172.31.66.231'])\n",
    "session = cluster.connect('sharelock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "def get_row_score(row):\n",
    "    user_score = 1\n",
    "    if (row['user_score'] > 1):\n",
    "        user_score = math.log(row['user_score'])\n",
    "    return (row['likes'] + row['retweets'] * 2 ) * user_score\n",
    "\n",
    "stop_ents = ['#', \"@\"]\n",
    "def is_valid_ent(ent, topic):\n",
    "    unspaced_ent = ent.replace(\" \", \"\")\n",
    "    phone_no_reg = '/^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$/im'\n",
    "    print(re.search(phone_no_reg, unspaced_ent))\n",
    "                        \n",
    "    stop_found = False\n",
    "    for stop in stop_ents:\n",
    "        if stop in ent:\n",
    "            stop_found = True\n",
    "    return ent != topic and not stop_found and len(ent) > 3\\\n",
    "               and not \"http\" in ent and not ent.replace(\" \", \"\").isdigit()\\\n",
    "               and not UNICODE_EMOJI in unspaced_ent\n",
    "\n",
    "def get_ent_dictionary(sorted_result, topic):\n",
    "    ent_dict = {}\n",
    "\n",
    "    for idx, row in sorted_result.iterrows():\n",
    "        text_item = row['tweet_text']\n",
    "        tweet_item = row\n",
    "        text_doc = nlp(row['tweet_text'])\n",
    "        for ent in text_doc.ents:\n",
    "            stripped_ent = ent.lemma_.strip()\n",
    "            if (is_valid_ent(stripped_ent, topic)):\n",
    "                if stripped_ent in ent_dict.keys():\n",
    "                    for t in ent_dict[stripped_ent]:\n",
    "                        tFound = False\n",
    "                        if t['tweet_id'] == tweet_item['tweet_id'] or nlp(t['tweet_text']).similarity(text_doc) > 0.95:\n",
    "                            tFound = True\n",
    "                            break   \n",
    "                        if not tFound:\n",
    "                            ent_dict[stripped_ent].append(tweet_item)\n",
    "                            \n",
    "                else:\n",
    "                    ent_dict[stripped_ent] = [tweet_item]               \n",
    "    return ent_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_tag_map(row, ent_dict):\n",
    "    post_tag_dict = []\n",
    "    stop_ents = ['#', \"@\"]\n",
    "\n",
    "    text_item = row['tweet_text']\n",
    "    tweet_item = row\n",
    "    text_doc = nlp(row['tweet_text'])\n",
    "    tweet_id = row['tweet_id']\n",
    "    entity_list = []\n",
    "    for key in ent_dict:\n",
    "        frame_list = ent_dict[key]\n",
    "        key_exists = False\n",
    "        for frame in frame_list:\n",
    "            if frame['tweet_id'] == tweet_id:\n",
    "                key_exists = True\n",
    "                break\n",
    "                if not key_exists:\n",
    "                    data = {}\n",
    "                    data[\"name\"] = key\n",
    "                    data[\"rank\"] = len(frame_list)\n",
    "                    post_tag_dict.append(data)\n",
    "    return post_tag_dict               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "topic_list_query = \"SELECT * from sharelock.topic_list\"\n",
    "topic_rows = session.execute(topic_list_query)\n",
    "topic_row_list = list(topic_rows)\n",
    "topic_frames = pd.DataFrame(topic_row_list)  \n",
    "\n",
    "for idx, frame in topic_frames.iterrows():\n",
    "    topic = frame['topic']\n",
    "    category = frame['category']\n",
    "    query = \"SELECT * from sharelock.active_tweets where topic='\"+topic+\"'order by inserted_at desc limit 2\"\n",
    "    rows = session.execute(query)\n",
    "    ent_dict = {}    \n",
    "    sorted_json = {}\n",
    "    xd = json.loads(rows[0].tweet_batch)\n",
    "    sorted_result = df = pd.DataFrame(data=xd) \n",
    "    if not sorted_result.empty:\n",
    "        ent_dict = get_ent_dictionary(sorted_result, topic)\n",
    "        for idx, row in sorted_result.iterrows():\n",
    "            post_ent_list = get_post_tag_map(row, ent_dict) \n",
    "            enity_score = 0\n",
    "            for ent in post_ent_list:\n",
    "                enity_score = enity_score + ent['rank']\n",
    "\n",
    "            SQL_QUERY = \"INSERT INTO sharelock.tweet_tags_with_rank (tweet_id , topic, category, entity_list, entity_score) values (?,?,?,?, ?)\"\n",
    "            params = [row['tweet_id'], topic, category, json.dumps(post_ent_list), enity_score]\n",
    "            prepared = session.prepare(SQL_QUERY)\n",
    "            session.execute(prepared, params)\n",
    "            # Create api endpoint for posts with entity rankings  \n",
    "\n",
    "        if ent_dict:\n",
    "            frame_score_list = {}\n",
    "            for key in ent_dict:\n",
    "                frame_score = 0\n",
    "                frame_list = ent_dict[key]\n",
    "                frame_tag = key\n",
    "                frame_tweet_list = []\n",
    "                if (len(frame_list) > 0):\n",
    "                    for frame in frame_list:\n",
    "                        frame_score = frame_score + frame['raw_score']\n",
    "                        frame_obj = {}\n",
    "                        raw_score = frame['raw_score']\n",
    "                        tweet_id = frame['tweet_id']\n",
    "                        tweet_text = frame['tweet_text']\n",
    "\n",
    "                        frame_tweet_list.append({\"id\": tweet_id, \"text\": tweet_text, \"score\": raw_score})\n",
    "\n",
    "                        insert_time = int(datetime.utcnow().timestamp())   \n",
    "                        insert_values = [topic, category, frame_tag, json.dumps(frame_tweet_list), frame_score, insert_time]\n",
    "                        sql_query = \"INSERT INTO sharelock.filtered_tag_frame (topic, category, tag, tweet_map, frame_raw_score, inserted_at) values (?, ?, ?, ?, ?, ?)\"\n",
    "                        try:\n",
    "                            prepared = session.prepare(sql_query)\n",
    "                            session.execute(prepared, (insert_values))\n",
    "                        except Exception as e:\n",
    "                            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
