{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_core.paths import jupyter_data_dir\n",
    "import constants\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "# update table save features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "import math\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect('sharelock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list_query = \"SELECT * from sharelock.topic_list\"\n",
    "topic_rows = session.execute(topic_list_query)\n",
    "topic_row_list = list(topic_rows)\n",
    "\n",
    "topic_frames = pd.DataFrame(topic_row_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = set()\n",
    "for idx, topic_item in topic_frames.iterrows():\n",
    "    categories.add(topic_item['category'])\n",
    "\n",
    "CATEGORY_LIST = list(categories) \n",
    "def get_sentence_score(tweet_doc):\n",
    "    # number of entitites * number of sentences\n",
    "    num_sents = len(list(tweet_doc.sents))\n",
    "    num_ents = len(list(tweet_doc.ents))\n",
    "    return num_sents * num_ents\n",
    "        \n",
    "    \n",
    "# number of hash-tags or mentions\n",
    "def get_tag_score(doc):\n",
    "    tag_score = 0\n",
    "    for token in doc:\n",
    "        if \"#\" in token.text or \"@\" in token.text:\n",
    "            tag_score = tag_score + 1 \n",
    "    return tag_score  \n",
    "\n",
    "\n",
    "upper_caps_stops = ['I']\n",
    "\n",
    "spam_word_stops = constants.SPAM_WORDS\n",
    "def get_structure_score(doc):\n",
    "    token_penalty = 0\n",
    "    for token in doc:\n",
    "        token_text = token.text\n",
    "        prev_token_caps = False\n",
    "        if token_text.isupper():\n",
    "            if prev_token_caps:\n",
    "                token_penalty = token_penalty + 10\n",
    "            prev_token_caps = True\n",
    "        else:\n",
    "            prev_token_caps = False\n",
    "        if token_text in upper_caps_stops:\n",
    "            token_penalty = token_penalty + 10\n",
    "        if token_text.lower() in spam_word_stops:\n",
    "            token_penalty = token_penalty + 10 \n",
    "    return token_penalty    \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "\n",
    "# get corpus of news text on topic\n",
    "def get_topic_web_corpus(category):\n",
    "    sql_str = \"SELECT * from sharelock.topic_link_data where link_category='\"+category+\"' allow filtering;\"\n",
    "    corpus_data = session.execute(sql_str)\n",
    "    topic_corpus = pd.DataFrame(list(corpus_data))\n",
    "    #print(len(topic_corpus))\n",
    "    topic_corpus['body_doc'] = topic_corpus['link_text'].apply(nlp)\n",
    "    topic_corpus['head_doc'] = topic_corpus['link_title'].apply(nlp)\n",
    "    return topic_corpus\n",
    "\n",
    "web_corpus_list = {}\n",
    "\n",
    "doc_title_list = []\n",
    "doc_body_list = []\n",
    "\n",
    "for category in CATEGORY_LIST:\n",
    "    web_corpus = get_topic_web_corpus(category)\n",
    "    web_corpus_list[category] = web_corpus\n",
    "\n",
    "    \n",
    "def get_body_relevance_score(sentence_doc, frame_category):\n",
    "    category_text_corpus = web_corpus_list[frame_category]\n",
    "\n",
    "    body_relevance_score = []\n",
    "    counter = 0\n",
    "    for corpus_body in category_text_corpus['body_doc']:\n",
    "        body_sents = corpus_body.sents\n",
    "        counter = counter + len(list(body_sents)) \n",
    "        for corpus_sentence in body_sents:\n",
    "            body_relevance_score.append(corpus_sentence.as_doc().similarity(sentence_doc))        \n",
    "    return body_relevance_score    \n",
    "\n",
    "\n",
    "def get_head_relevance_score(sentence_doc, frame_category):\n",
    "    category_text_corpus = web_corpus_list[frame_category]\n",
    "\n",
    "    head_relevance_score = []\n",
    "    counter = 0\n",
    "    for corpus_head in category_text_corpus['head_doc']:\n",
    "        head_sents = corpus_head.sents\n",
    "        counter = counter + len(list(head_sents))   \n",
    "        for corpus_sentence in head_sents:\n",
    "            head_relevance_score.append(corpus_sentence.as_doc().similarity(sentence_doc))       \n",
    "    return head_relevance_score  \n",
    "    \n",
    "def get_raw_score(frame):\n",
    "    user_score = 1\n",
    "    if frame['user_score'] > 1:\n",
    "        user_score = math.log(frame['user_score'])\n",
    "    base_score = frame['likes'] + frame['retweets'] * 2\n",
    "    if base_score < 1:\n",
    "        base_score = 1\n",
    "    return base_score * user_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_features_matrix(feature_matrix_1, feature_matrix_2):\n",
    "    p\n",
    "    np_array_1 = feature_matrix_1.toarray()\n",
    "    np_array_2 = feature_matrix_2.toarray()\n",
    "\n",
    "    new_array = np.concatenate((np_array_1, np_array_2), axis=1)\n",
    "\n",
    "    return sparse.csr_matrix(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "time_now = datetime.now()\n",
    "\n",
    "time_frame = time_now - timedelta(minutes = 30)\n",
    "time_frame = '{0:%Y-%m-%d %H:%M:%S}'.format(time_frame)\n",
    "\n",
    "query = \"SELECT * from sharelock.streaming_tweets where inserted_at>'\" + time_frame + \"' allow filtering\"\n",
    "\n",
    "rows = session.execute(query)\n",
    "row_list = list(rows)\n",
    "result = []\n",
    "\n",
    "if len(row_list) > 0:\n",
    "    frame_result = pd.DataFrame(row_list)        \n",
    "def start_classify_batch():\n",
    "    for idx, topic_item in topic_frames.iterrows():\n",
    "        category = topic_item['category']\n",
    "        topic = topic_item['topic']\n",
    "        result = frame_result.query(\"topic=='\"+topic+\"'\")  \n",
    "        result.set_index('tweet_id')\n",
    "        result = result.drop_duplicates(subset='tweet_id', keep='last')\n",
    "        print(category)\n",
    "        time_frame = time_now - timedelta(minutes = 22)\n",
    "        result = result[result['topic'] == topic]\n",
    "        input_frame = result.sort_values(by=['tweet_time'], ascending=False)\n",
    "        input_frame['tweet_time'] = pd.to_datetime(input_frame['tweet_time'])\n",
    "        input_frame = input_frame[(input_frame['inserted_at'] > time_frame)]\n",
    "\n",
    "        if not input_frame.empty:    \n",
    "            input_frame['text_tokens'] = input_frame['tweet_text'].apply(nlp)\n",
    "            input_frame['sentence_score'] = input_frame['text_tokens'].apply(get_sentence_score)\n",
    "            input_frame['tag_score'] = input_frame['text_tokens'].apply(get_tag_score)\n",
    "            input_frame['structure_score'] = input_frame['text_tokens'].apply(get_structure_score)\n",
    "            input_frame['raw_score'] = input_frame.apply(get_raw_score, axis=1)\n",
    "            input_frame['body_relevance_score'] = input_frame['text_tokens'].apply(get_body_relevance_score, args = (category,))\n",
    "            input_frame['head_relevance_score'] = input_frame['text_tokens'].apply(get_head_relevance_score, args = (category,))\n",
    "\n",
    "\n",
    "            mapper_model = joblib.load(\"%s-mapper.pkl\" % category)\n",
    "            feature_vector = mapper_model.transform(input_frame)   \n",
    "            head_relevance_matrix = input_frame['body_relevance_score'].values\n",
    "            body_relevance_matrix = input_frame['head_relevance_score'].values\n",
    "            final_features = []\n",
    "\n",
    "            for idx, f in enumerate(feature_vector):\n",
    "                final_features.append(list(feature_vector[idx]) + list(head_relevance_matrix[idx]) + list(body_relevance_matrix[idx]))\n",
    "\n",
    "            soft_relevant_classifier = joblib.load(\"%s-relevance-classifier.pkl\" % category)\n",
    "            soft_spam_classifier = joblib.load(\"%s-spam-classifier.pkl\" % category)\n",
    "            input_frame[\"relevant_prediction\"] = soft_relevant_classifier.predict(final_features)\n",
    "            input_frame[\"spam_prediction\"] = soft_spam_classifier.predict(final_features)\n",
    "\n",
    "            input_frame = input_frame.query('relevant_prediction==1').query('spam_prediction==0')\n",
    "            input_frame['is_graded'] = input_frame['is_graded'].astype('int')\n",
    "            input_frame['inserted_at'] = input_frame['inserted_at']\n",
    "            input_frame['raw_score'] = input_frame['raw_score'].astype('int')\n",
    "\n",
    "            df_json = input_frame[['tweet_text', 'tweet_id', 'raw_score', 'user_score', 'likes', 'retweets', 'tweet_time', 'links']].to_json(orient = \"records\")\n",
    "            query = \"INSERT INTO sharelock.active_tweets (topic, inserted_at, category, tweet_batch) values (?, ?, ?, ?)\";\n",
    "            nvals = [topic, time_now.timestamp(), category, df_json]\n",
    "\n",
    "            prepared = session.prepare(query)\n",
    "            session.execute(prepared, (nvals))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaming\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "crypto\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "software\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "drone\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "gaming\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "software\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n",
      "crypto\n",
      "['tweet_id', 'inserted_at', 'category', 'is_classified_spam', 'is_graded', 'is_relevant', 'is_spam', 'likes', 'links', 'node_type', 'retweets', 'topic', 'tweet_body', 'tweet_text', 'tweet_time', 'tweeted_at', 'user', 'user_score', 'text_tokens', 'sentence_score', 'tag_score', 'structure_score', 'raw_score', 'body_relevance_score', 'head_relevance_score', 'relevant_prediction', 'spam_prediction']\n"
     ]
    }
   ],
   "source": [
    "start_classify_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
