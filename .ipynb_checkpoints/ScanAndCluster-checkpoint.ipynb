{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import operator\n",
    "from datetime import datetime\n",
    "import textacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect('sharelock')\n",
    "en = textacy.load_spacy('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text rows, lemmatize, remove stop words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def is_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += s.count(emoji)\n",
    "        if count > 1:\n",
    "            return False\n",
    "    return bool(count)\n",
    "\n",
    "def get_tweet_tokens_list(tweet):\n",
    "    t_doc = nlp(tweet)\n",
    "    token_list = []\n",
    "    for token in t_doc:\n",
    "        if token.is_alpha and not token.is_stop and \"-PRON-\" not in token.lemma_ and not is_emoji(token.lemma_):\n",
    "            token_list.append(token.lemma_.strip()) \n",
    "    return token_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def get_tf_idf_vector(text_token_list):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy_fun,\n",
    "        preprocessor=dummy_fun,\n",
    "        token_pattern=None)  \n",
    "    pos_vectorized = tfidf.fit_transform(text_token_list)\n",
    "    return pos_vectorized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_vector(tweet_text_list):\n",
    "    similarity_list = []\n",
    "    for tweet1 in tweet_text_list:\n",
    "        sim_row = []\n",
    "        for tweet2 in tweet_text_list:\n",
    "            sim_row.append(tweet1.similarity(tweet2))\n",
    "        similarity_list.append(sim_row)\n",
    "    return  similarity_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_raw_score(frame):\n",
    "    user_score = 1\n",
    "    if frame['user_score'] > 1:\n",
    "        user_score = math.log(frame['user_score'])\n",
    "    base_score = frame['likes'] + frame['retweets'] * 2\n",
    "    if base_score < 1:\n",
    "        base_score = 1\n",
    "    return base_score * user_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stop_ents = [\"@\"]\n",
    "def is_valid_ent(ent, topic):\n",
    "    unspaced_ent = ent.replace(\" \", \"\")\n",
    "    phone_no_reg = '/^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$/im'                        \n",
    "    stop_found = False\n",
    "    num_word_in_ent = len(ent.split())\n",
    "    for stop in stop_ents:\n",
    "        if stop in ent:\n",
    "            stop_found = True\n",
    "    return ent != topic and not stop_found and len(ent) > 3\\\n",
    "               and not \"http\" in ent and not ent.replace(\" \", \"\").isdigit() and \"amp\" not in ent and ent != '-PRON-'\\\n",
    "               and num_word_in_ent <= 3 \n",
    "\n",
    "def get_ent_freq_score(vec, topic):\n",
    "    ent_list = []\n",
    "    for row in vec:\n",
    "        for vec_ent in row:\n",
    "            if (is_valid_ent(vec_ent.lemma_, topic)):\n",
    "                ent_list.append(vec_ent.lemma_)\n",
    "    return ent_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_match_vector(frame, entity_list, topic):\n",
    "    entity_matrix = []\n",
    "    for row in frame:\n",
    "        row_entity_list = []\n",
    "        for row_ent in row.ents:\n",
    "            if is_valid_ent(row_ent.lemma_, topic):\n",
    "                row_entity_list.append(row_ent.lemma_.strip())\n",
    "        match_ent_vector = []                       \n",
    "        for ent in entity_list:\n",
    "            if (ent in row_entity_list):\n",
    "                match_ent_vector.append(1)\n",
    "            else:\n",
    "                match_ent_vector.append(0)\n",
    "                \n",
    "        entity_matrix.append(match_ent_vector)\n",
    "    return entity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_ents(doc, topic):\n",
    "    ent_set = set()\n",
    "    for row_ent in doc.ents:\n",
    "        if is_valid_ent(row_ent.lemma_, topic):\n",
    "            ent_set.add(row_ent.lemma_.strip())\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "            ent_set.add(token.lemma_)\n",
    "            \n",
    "    for chunk in doc.noun_chunks:\n",
    "        ent_set.add(chunk.lemma_.strip())\n",
    "        ent_set.add(chunk.root.lemma_.strip())\n",
    "        #ent_set.add(chunk.root.head.lemma_.strip())\n",
    "    if \"-PRON-\" in ent_set:\n",
    "        ent_set.remove(\"-PRON-\")\n",
    "    return list(ent_set)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_ent_list(frame, entity_list, topic):\n",
    "    r_ents = {}\n",
    "    for row in frame:\n",
    "        if is_valid_ent(row_ent.lemma_, topic):\n",
    "            curr = row_ent.lemma_.strip()\n",
    "            if r_ents[curr]:\n",
    "                r_ents[curr] = r_ents[curr] + 1\n",
    "            else:\n",
    "                r_ents[curr] = 1         \n",
    "    return r_ents            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_posts(sorted_result):\n",
    "    for idx1, frame1 in sorted_result.iterrows():\n",
    "        doc1 = frame1['tweet_clean_tokens']\n",
    "        for idx2, frame2 in  sorted_result.iterrows():\n",
    "            doc2 = frame2['tweet_clean_tokens']\n",
    "            if idx1 != idx2 and doc1.similarity(doc2) > 0.99:\n",
    "                sorted_result.drop(idx2, inplace=True)\n",
    "    return sorted_result           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank\n",
    "import sys\n",
    "import json\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "def process_text_rank_for_corpus(topic_dict):\n",
    "    tweet_text_corpus = \"\"\n",
    "    for t in topic_dict:\n",
    "        topic_dict[t].sort(key=lambda x: x['prob'], reverse=True)\n",
    "        for tweet_row in topic_dict[t][2:6]:\n",
    "            tweet_text_corpus = tweet_text_corpus + tweet_row['tweet_clean_text']\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect\n",
    "RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def strip_emoji(text):\n",
    "    return RE_EMOJI.sub(r'', text)\n",
    "    return tweet_text\n",
    "\n",
    "def get_cleaned_text(tweet_text):\n",
    "    tweet_text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", tweet_text)\n",
    "    tweet_text = strip_emoji(tweet_text)\n",
    "    return tweet_text   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_caps_stops = ['I']\n",
    "import constants\n",
    "\n",
    "spam_word_stops = constants.SPAM_WORDS\n",
    "\n",
    "def get_structure_score(doc):\n",
    "    token_penalty = 0\n",
    "    for token in doc:\n",
    "        token_text = token.text\n",
    "        prev_token_caps = False\n",
    "        if \"#\" in token_text or \"@\" in token_text or \"...\" in token_text:\n",
    "            token_penalty = token_penalty + 10\n",
    "        if token_text.isupper() and token_text not in upper_caps_stops:\n",
    "            if prev_token_caps:\n",
    "                token_penalty = token_penalty + 10\n",
    "            prev_token_caps = True\n",
    "        else:\n",
    "            prev_token_caps = False\n",
    "    \n",
    "    return token_penalty    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_weights(tweet, tweet_terms):\n",
    "    tweet_text = tweet['tweet_text']\n",
    "    tt_doc = nlp(tweet_text)\n",
    "    sentence_scores = []\n",
    "    for sent in tt_doc.sents:\n",
    "        ent_score = 0\n",
    "        sent_text = sent.text\n",
    "        for term in tweet_terms:\n",
    "            if term['term'] in sent.lemma_:\n",
    "                ent_score = ent_score + ((term['weight'] + 1)* 10)\n",
    "        sent_doc = sent.as_doc()   \n",
    "        num_words = 0\n",
    "        for token in sent_doc:\n",
    "            if token.is_alpha and not token.is_stop and not token.text.isupper() and not \"#\" in token.text:\n",
    "                num_words = num_words + 1\n",
    "\n",
    "        word_score = 0\n",
    "        if num_words >= 6 and num_words <= 30:\n",
    "            word_score = 100\n",
    "\n",
    "        str_score = get_structure_score(sent_doc)\n",
    "        grammar_score = 0\n",
    "        \n",
    "        if list(textacy.extract.pos_regex_matches(sent, r\"<DET>? (<NOUN>+ <ADP|CONJ>)* <NOUN>+\")):\n",
    "            grammar_score = grammar_score + 100\n",
    "        elif list(textacy.extract.pos_regex_matches(sent, r\"<VERB>?<ADV>*<VERB>+\")):\n",
    "            grammar_score = grammar_score + 50\n",
    "        elif list(textacy.extract.pos_regex_matches(sent, r\"<PREP> <DET>? (<NOUN>+<ADP>)* <NOUN>+\")):\n",
    "            grammar_score = grammar_score + 20\n",
    "        \n",
    "        nc = textacy.extract.noun_chunks(tt_doc)\n",
    "        grammar_score = grammar_score + (len(list(nc)))\n",
    "        \n",
    "        nv_triples = textacy.extract.subject_verb_object_triples(tt_doc)\n",
    "        nv_grams =  textacy.extract.ngrams(tt_doc, 3, filter_stops=True, filter_punct=True, filter_nums=False, include_pos=None, exclude_pos=None, min_freq=1)\n",
    "\n",
    "        grammar_score = grammar_score + (len(list(nv_triples)))\n",
    "        grammar_score = grammar_score + (len(list(nv_grams)))\n",
    "        \n",
    "        user_normalized_score = 1\n",
    "        user_score = int(tweet['user_score'])\n",
    "        if user_score > 1:\n",
    "            user_normalized_score = int(math.log(user_score))\n",
    "        \n",
    "        final_score_str = str(word_score) + str(user_normalized_score) + str(int(ent_score)) + str(grammar_score)\n",
    "\n",
    "        \n",
    "        sentence_scores.append({\"text\": strip_emoji(re.sub(r\"(?:\\https?\\://)\\S+\", \"\", sent.text)) , \"ent_score\": ent_score, \"grammar_score\": grammar_score,\n",
    "                                \"word_score\": word_score, \"structure_penalty\": str_score, \"final_score\": final_score_str})\n",
    "    return sentence_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import textacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def start_cluster_batch():\n",
    "    topic_list_query = \"SELECT * from sharelock.topic_list\"\n",
    "    topic_rows = session.execute(topic_list_query)\n",
    "    topic_row_list = list(topic_rows)\n",
    "    topic_frames = pd.DataFrame(topic_row_list)\n",
    "    for idx, frame in topic_frames.iterrows():\n",
    "        topic = frame['topic']\n",
    "        category = frame['category']\n",
    "        query = \"SELECT * from sharelock.active_tweets where topic='\"+topic+\"'order by inserted_at desc limit 30\"\n",
    "        rows = session.execute(query)\n",
    "        ent_dict = {}    \n",
    "        sorted_json = {}\n",
    "\n",
    "        row_list = []\n",
    "        for row in rows:\n",
    "            xd = json.loads(row.tweet_batch)\n",
    "            row_list = row_list + xd\n",
    "\n",
    "        sorted_result = df = pd.DataFrame(data=row_list) \n",
    "        sorted_result.set_index('tweet_id')\n",
    "        sorted_result = sorted_result.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "        # Clean results by dropping items with similarity score o.98 or higher\n",
    "\n",
    "        sorted_result['tweet_tokens'] = sorted_result['tweet_text'].apply(nlp)\n",
    "        sorted_result['tweet_clean_text'] = sorted_result['tweet_text'].apply(get_cleaned_text)\n",
    "        sorted_result['tweet_clean_tokens'] = sorted_result['tweet_clean_text'].apply(nlp)\n",
    "        sorted_result = remove_duplicate_posts(sorted_result)\n",
    "\n",
    "        corpus = textacy.Corpus(lang=\"en_core_web_lg\", texts = list(sorted_result['tweet_text']), metadatas=list(sorted_result['tweet_id']))\n",
    "\n",
    "\n",
    "        terms_list = (doc.to_terms_list(ngrams=(1, 2, 3), named_entities=True, normalize=u'lemma', lemmatize=True, lowercase=True, as_strings=True,\\\n",
    "                                        filter_stops=True, filter_punct=True, min_freq=1, exclude_pos=(\"PRON\", \"X\", \"PUNCT\", \"SYM\"))\n",
    "                      for doc in corpus)\n",
    "  \n",
    "        vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth')\n",
    "\n",
    "        textacy.text_utils.clean_terms(terms_list)\n",
    "    \n",
    "        doc_term_matrix = vectorizer.fit_transform(terms_list)\n",
    "        \n",
    "        num_topics = int(len(sorted_result)/10)\n",
    "\n",
    "        model = textacy.tm.TopicModel('nmf', n_topics=num_topics)\n",
    "        model.fit(doc_term_matrix)    \n",
    "\n",
    "        doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "\n",
    "        topic_cluster = {}\n",
    "        for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=-1, top_n=8, weights=True):\n",
    "            dct = dict(top_terms)\n",
    "            tt_list = []\n",
    "            for j in dct.keys():\n",
    "                    tt_list.append({\"term\":j, \"weight\": dct[j]})\n",
    "            topic_cluster[\"topic-\"+str(topic_idx)] = {\"terms\": tt_list}\n",
    "\n",
    "        for topic_idx, top_docs in model.top_topic_docs(doc_topic_matrix, topics=-1, top_n=6, weights=True):\n",
    "            dct = dict(top_docs)\n",
    "            tweet_in_topic_list = []\n",
    "            for j in dct.keys():\n",
    "                query_str = \"tweet_id=\"+ corpus[j].metadata\n",
    "                curr = sorted_result[sorted_result['tweet_id']==corpus[j].metadata]\n",
    "                curr_frame_row = curr.iloc[0]\n",
    "                is_attached_to_topic = False\n",
    "                for prev_topic in topic_cluster:\n",
    "                    if 'tweets' in topic_cluster[prev_topic]:\n",
    "                        tweet_list = topic_cluster[prev_topic]['tweets']\n",
    "                        for tweet in tweet_list:\n",
    "                            if tweet['tweet_id'] == curr.iloc[0]['tweet_id']:\n",
    "                                is_attached_to_topic = True\n",
    "                                break\n",
    "\n",
    "                if not is_attached_to_topic:\n",
    "                    tweet_in_topic_list.append({\"tweet_id\": curr.iloc[0]['tweet_id'], \"tweet_text\": curr.iloc[0]['tweet_text'],\n",
    "                                            \"user_score\": str(curr.iloc[0]['user_score']), \"raw_score\": str(curr.iloc[0]['raw_score'])})\n",
    "            if tweet_in_topic_list:\n",
    "                topic_cluster[\"topic-\"+str(topic_idx)]['tweets'] = tweet_in_topic_list\n",
    "\n",
    "        for curr_topic in topic_cluster:\n",
    "            if 'tweets' in topic_cluster[curr_topic]:\n",
    "                sent_weights = []\n",
    "                for tweet in topic_cluster[curr_topic]['tweets']:\n",
    "                    sent_weights = sent_weights + get_sent_weights(tweet, topic_cluster[curr_topic]['terms'])\n",
    "                sent_weights = sorted(sent_weights, key=lambda x: x['final_score'], reverse=True)\n",
    "                top_sents =  sent_weights[0:2]\n",
    "                sorted_top_sents = sorted(sent_weights, key=lambda x: x['ent_score'], reverse=True)\n",
    "                topic_title = \"\"\n",
    "                topic_title_list = []\n",
    "                for sent in sorted_top_sents:\n",
    "                    if sent['structure_penalty'] < 50 and sent['word_score'] > 0:\n",
    "                        topic_title_list.append(sent['text'].strip('\\n'))           \n",
    "                topic_cluster[curr_topic]['title'] =  topic_title_list\n",
    "        \n",
    "        result_dict = {}\n",
    "        for k in topic_cluster.keys():\n",
    "            if 'tweets' in topic_cluster[k]:\n",
    "                 result_dict[k] = topic_cluster[k]\n",
    "            \n",
    "\n",
    "        insert_at = datetime.datetime.now().timestamp()\n",
    "\n",
    "        insert_values = [topic, category, insert_at, json.dumps(result_dict)]\n",
    "\n",
    "        sql_query = \"INSERT into sharelock.topic_clusters (topic, category, inserted_at, tweet_cluster) values (?, ?, ?, ?)\"\n",
    "        try:\n",
    "            prepared = session.prepare(sql_query)\n",
    "            session.execute(prepared, (insert_values))\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ml_conda/lib/python3.5/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/ml_conda/lib/python3.5/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/ml_conda/lib/python3.5/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/envs/ml_conda/lib/python3.5/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "start_cluster_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
